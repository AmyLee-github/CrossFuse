{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c20cef00-b42c-4c70-9b6e-e0d5328b5a64",
   "metadata": {},
   "source": [
    "'''\n",
    "@Author  :   Hui Li, Jiangnan University\n",
    "@Contact :   lihui.cv@jiangnan.edu.cn\n",
    "@File    :   transformer_cam.py\n",
    "@Time    :   2023/03/30 18:00:20\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db744c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from tools.utils import vision_features, save_image_heat_map, save_image_heat_map_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579fd70d",
   "metadata": {},
   "source": [
    "**模块架构图：**  \n",
    "  \n",
    "<img src=\"../ipynb_imgs/transformer_cam.jpg\" style=\"zoom:70%\" />\n",
    "\n",
    "**类之间的关系图：**  \n",
    "cross_encoder  \n",
    "  ├── self_atten  \n",
    "  │     ├── PatchEmbed_tensor  \n",
    "  │     │     └── Padding_tensor  \n",
    "  │     ├── self_atten_module  \n",
    "  │     │     └── Block  \n",
    "  │     │           ├── Attention  \n",
    "  │     │           └── MLP  \n",
    "  │     └── Recons_tensor  \n",
    "  ├── cross_atten  \n",
    "  │     ├── PatchEmbed_tensor  \n",
    "  │     │     └── Padding_tensor  \n",
    "  │     ├── cross_atten_module  \n",
    "  │     │     └── Block  \n",
    "  │     │           ├── Attention  \n",
    "  │     │           └── MLP  \n",
    "  │     └── Recons_tensor  \n",
    "  └── Recons_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b06d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding_tensor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Padding_tensor, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        h_patches = int(np.ceil(h / self.patch_size))\n",
    "        w_patches = int(np.ceil(w / self.patch_size))\n",
    "\n",
    "        h_padding = np.abs(h - h_patches * self.patch_size)\n",
    "        w_padding = np.abs(w - w_patches * self.patch_size)\n",
    "        # TODO: 改成左右上下等量padding\n",
    "        reflection_padding = [0, w_padding, 0, h_padding]\n",
    "        reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        x = reflection_pad(x)\n",
    "        return x, [h_patches, w_patches, h_padding, w_padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7335b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed_tensor(nn.Module):\n",
    "    def __init__(self, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.padding_tensor = Padding_tensor(patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x, patches_paddings = self.padding_tensor(x)\n",
    "        h_patches = patches_paddings[0]\n",
    "        w_patches = patches_paddings[1]\n",
    "        # -------------------------------------------\n",
    "        patch_matrix = None\n",
    "        for i in range(h_patches):\n",
    "            for j in range(w_patches):\n",
    "                patch_one = x[:, :, i * self.patch_size: (i + 1) * self.patch_size,\n",
    "                            j * self.patch_size: (j + 1) * self.patch_size]\n",
    "                # patch_one = patch_one.flatten(1)\n",
    "                # patch_one = patch_one.unsqueeze(2)\n",
    "                patch_one = patch_one.reshape(-1, c, 1, self.patch_size, self.patch_size)\n",
    "                if i == 0 and j == 0:\n",
    "                    patch_matrix = patch_one\n",
    "                else:\n",
    "                    patch_matrix = torch.cat((patch_matrix, patch_one), dim=2)\n",
    "        # patch_matrix  # (b, c, N, patch_size, patch_size)\n",
    "        return patch_matrix, patches_paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recons_tensor(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, patches_tensor, patches_paddings):\n",
    "        B, C, N, Ph, Pw = patches_tensor.shape\n",
    "        h_patches = patches_paddings[0]\n",
    "        w_patches = patches_paddings[1]\n",
    "        h_padding = patches_paddings[2]\n",
    "        w_padding = patches_paddings[3]\n",
    "        assert N == h_patches * w_patches, \\\n",
    "            f\"The number of patches ({N}) doesn't match the Patched_embed operation ({h_patches}*{w_patches}).\"\n",
    "        assert Ph == self.patch_size and Pw == self.patch_size, \\\n",
    "            f\"The size of patch tensor ({Ph}*{Pw}) doesn't match the patched size ({self.patch_size}*{self.patch_size}).\"\n",
    "\n",
    "        patches_tensor = patches_tensor.view(-1, C, N, self.patch_size, self.patch_size)\n",
    "        # ----------------------------------------\n",
    "        pic_all = None\n",
    "        for i in range(h_patches):\n",
    "            pic_c = None\n",
    "            for j in range(w_patches):\n",
    "                if j == 0:\n",
    "                    pic_c = patches_tensor[:, :, i * w_patches + j, :, :]\n",
    "                else:\n",
    "                    pic_c = torch.cat((pic_c, patches_tensor[:, :, i * w_patches + j, :, :]), dim=3)\n",
    "            if i == 0:\n",
    "                pic_all = pic_c\n",
    "            else:\n",
    "                pic_all = torch.cat((pic_all, pic_c), dim=2)\n",
    "        b, c, h, w = pic_all.shape\n",
    "        pic_all = pic_all[:, :, 0:(h-h_padding), 0:(w-w_padding)]\n",
    "        return pic_all\n",
    "# -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x + 1e-6\n",
    "        x = self.fc1(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches + 1, hidden_features)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ee9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self or cross attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=16, qkv_bias=True, attn_p=0., proj_p=0., cross=False):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # self.recons_tensor = Recons_tensor(2)\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.cross = cross\n",
    "        if cross:\n",
    "            self.q_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.k_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "            self.v_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.cross:\n",
    "            n_samples, n_tokens, dim = x[0].shape\n",
    "            if dim != self.dim:\n",
    "                raise ValueError\n",
    "\n",
    "            n_tokens_en = n_tokens\n",
    "            q = self.q_linear(x[0]).reshape(n_samples, n_tokens, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "            k = self.k_linear(x[1]).reshape(n_samples, n_tokens_en, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "            v = self.v_linear(x[2]).reshape(n_samples, n_tokens_en, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            n_samples, n_tokens, dim = x.shape\n",
    "            if dim != self.dim:\n",
    "                raise ValueError\n",
    "\n",
    "            qkv = self.qkv(x)  # (n_samples, n_patches, 3 * dim)\n",
    "            qkv = qkv.reshape(\n",
    "                n_samples, n_tokens, 3, self.n_heads, self.head_dim\n",
    "            )  # (n_smaples, n_patches, 3, n_heads, head_dim)\n",
    "            qkv = qkv.permute(\n",
    "                2, 0, 3, 1, 4\n",
    "            )  # (3, n_samples, n_heads, n_patches, head_dim)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1)  # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "        dp = (q @ k_t) * self.scale  # (n_samples, n_heads, n_patches, n_patches)\n",
    "        # exp(-x)\n",
    "        # dp 过大，softmax之后数值可能溢出\n",
    "        if self.cross:\n",
    "            # t_str = time.time()\n",
    "            # dp_s = dp.softmax(dim=-1)\n",
    "            # vision_features(dp_s, 'atten', 'dp_'+str(t_str))\n",
    "            dp = -1 * dp\n",
    "            # attn = dp.softmax(dim=-1)\n",
    "            # vision_features(attn, 'atten', 'dp_v_'+str(t_str))\n",
    "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches, n_patches)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches +1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)  # (n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        x = self.proj(weighted_avg)  # (n_samples, n_patches + 1, dim)\n",
    "        x = self.proj_drop(x)  # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "        # if self.cross:\n",
    "        #     x_temp = x.view(1, 256, 128, 2, 2).permute(0, 2, 1, 3, 4)\n",
    "        #     x_temp = self.recons_tensor(x_temp, [16,16,0,0])  # B, C, H, W\n",
    "        #     vision_features(x_temp, 'atten', 'attn_x')\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0., cross=False):\n",
    "        super().__init__()\n",
    "        self.cross = cross\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            n_heads=n_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_p=attn_p,\n",
    "            proj_p=p,\n",
    "            cross=cross\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features=dim,\n",
    "            hidden_features=hidden_features,\n",
    "            out_features=dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cross:\n",
    "            x_ = [self.norm1(_x) for _x in x]\n",
    "            # x_ = x\n",
    "            out = x[2] + self.attn(x_)\n",
    "            out = out + self.mlp(self.norm2(out))\n",
    "            out = [x_[0], out, out]\n",
    "        else:\n",
    "            out = x + self.attn(self.norm1(x))\n",
    "            out = out + self.mlp(self.norm2(out))\n",
    "        \n",
    "        return out\n",
    "# --------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae35ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_atten_module(nn.Module):\n",
    "    def __init__(self, embed_dim, num_p, depth, n_heads=16,\n",
    "                 mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(dim=embed_dim, n_heads=n_heads,\n",
    "                      mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p, cross=False)\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # x_ori = x_in\n",
    "        x = x_in\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x_self = x\n",
    "        # x_self = x_in + x\n",
    "        return x_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_atten_module(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches, depth, n_heads=16,\n",
    "                 mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p,\n",
    "                      cross=True)\n",
    "                if i == 0 else\n",
    "                Block(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p,\n",
    "                      cross=True)\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x1_ori, x2_ori):\n",
    "        x1 = x1_ori\n",
    "        x2 = x2_ori\n",
    "        x2 = self.pos_drop(x2)\n",
    "        x = [x1, x2, x2]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            x[2] = self.norm(x[2])\n",
    "        x_self = x[2]\n",
    "        # x_self = x2_ori + x[2]\n",
    "        return x_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_atten(nn.Module):\n",
    "    def __init__(self, patch_size, embed_dim, num_patches, depth_self, n_heads=16,\n",
    "                 mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_embed_tensor = PatchEmbed_tensor(patch_size)\n",
    "        self.recons_tensor = Recons_tensor(patch_size)\n",
    "        self.self_atten1 = self_atten_module(embed_dim, num_patches, depth_self,\n",
    "                                              n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "        self.self_atten2 = self_atten_module(embed_dim, num_patches, depth_self,\n",
    "                                                   n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "\n",
    "    def forward(self, x1, x2, last=False):\n",
    "        # patch\n",
    "        x_patched1, patches_paddings = self.patch_embed_tensor(x1)\n",
    "        # B, C, N, Ph, Pw = x_patched1.shape\n",
    "        x_patched2, _ = self.patch_embed_tensor(x2)\n",
    "        # B, C, N, Ph, Pw = x_patched1.shape\n",
    "        b, c, n, h, w = x_patched1.shape\n",
    "        # b, n, c*h*w\n",
    "        x_patched1 = x_patched1.transpose(2, 1).contiguous().view(b, n, c * h * w)\n",
    "        x_patched2 = x_patched2.transpose(2, 1).contiguous().view(b, n, c * h * w)\n",
    "        x1_self_patch = self.self_atten1(x_patched1)\n",
    "        x2_self_patch = self.self_atten2(x_patched2)\n",
    "       \n",
    "        # reconstruct\n",
    "        if last is False:\n",
    "            x1_self_patch = x1_self_patch.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "            x_self1 = self.recons_tensor(x1_self_patch, patches_paddings)  # B, C, H, W\n",
    "            x2_self_patch = x2_self_patch.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "            x_self2 = self.recons_tensor(x2_self_patch, patches_paddings)  # B, C, H, W\n",
    "        else:\n",
    "            x_self1 = x1_self_patch\n",
    "            x_self2 = x2_self_patch\n",
    "\n",
    "        return x_self1, x_self2, patches_paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_atten(nn.Module):\n",
    "    def __init__(self, patch_size, embed_dim, num_patches, depth_self, depth_cross, n_heads=16,\n",
    "                 mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_embed_tensor = PatchEmbed_tensor(patch_size)\n",
    "        self.recons_tensor = Recons_tensor(patch_size)\n",
    "        \n",
    "        self.cross_atten1 = cross_atten_module(embed_dim, num_patches, depth_cross,\n",
    "                                                     n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "        self.cross_atten2 = cross_atten_module(embed_dim, num_patches, depth_cross,\n",
    "                                                     n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "        # self.cross_atten = patch_cross_atten_module(img_size, patch_size, embed_dim, num_patches, depth_cross,\n",
    "        #                                              n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "\n",
    "    def forward(self, x1, x2, patches_paddings):\n",
    "        # patch\n",
    "        x_patched1, patches_paddings = self.patch_embed_tensor(x1)\n",
    "        # B, C, N, Ph, Pw = x_patched1.shape\n",
    "        x_patched2, _ = self.patch_embed_tensor(x2)\n",
    "        # B, C, N, Ph, Pw = x_patched1.shape\n",
    "        b, c, n, h, w = x_patched1.shape\n",
    "        # b, n, c*h*w\n",
    "        x1_self_patch = x_patched1.transpose(2, 1).contiguous().view(b, n, c * h * w)\n",
    "        x2_self_patch = x_patched2.transpose(2, 1).contiguous().view(b, n, c * h * w)\n",
    "        \n",
    "        x_in1 = x1_self_patch\n",
    "        x_in2 = x2_self_patch\n",
    "        cross1 = self.cross_atten1(x_in1, x_in2)\n",
    "        cross2 = self.cross_atten2(x_in2, x_in1)\n",
    "        out = cross1 + cross2\n",
    "        \n",
    "        # reconstruct\n",
    "        x1_self_patch = x1_self_patch.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "        x_self1 = self.recons_tensor(x1_self_patch, patches_paddings)  # B, C, H, W\n",
    "        x2_self_patch = x2_self_patch.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "        x_self2 = self.recons_tensor(x2_self_patch, patches_paddings)  # B, C, H, W\n",
    "        \n",
    "        cross1 = cross1.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "        cross1_all = self.recons_tensor(cross1, patches_paddings)  # B, C, H, W\n",
    "        \n",
    "        cross2 = cross2.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "        cross2_all = self.recons_tensor(cross2, patches_paddings)  # B, C, H, W\n",
    "        \n",
    "        out = out.view(b, n, c, h, w).permute(0, 2, 1, 3, 4)\n",
    "        out_all = self.recons_tensor(out, patches_paddings)  # B, C, H, W\n",
    "        \n",
    "        return out_all, x_self1, x_self2, cross1_all, cross2_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cross_encoder(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_dim, num_patches, depth_self, depth_cross, n_heads=16,\n",
    "                 mlp_ratio=4., qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.shift_size = int(img_size / 2)\n",
    "        self.depth_cross = depth_cross\n",
    "        # self.depth_cross = 0\n",
    "\n",
    "        self.self_atten_block1 = self_atten(self.patch_size, embed_dim, num_patches, depth_self,\n",
    "                                              n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "        self.self_atten_block2 = self_atten(self.patch_size, embed_dim, num_patches, depth_self,\n",
    "                                                   n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "        \n",
    "        self.cross_atten_block = cross_atten(self.patch_size, embed_dim, self.num_patches, depth_self,\n",
    "                                               depth_cross, n_heads, mlp_ratio, qkv_bias, p, attn_p)\n",
    "\n",
    "    def forward(self, x1, x2, shift_flag=True):\n",
    "        # x1 -->> ir, x2 -->> vi\n",
    "        # self-attention\n",
    "        x1_atten, x2_atten, paddings = self.self_atten_block1(x1, x2)\n",
    "        x1_a, x2_a = x1_atten, x2_atten\n",
    "        # shift\n",
    "        if shift_flag:\n",
    "            shifted_x1 = torch.roll(x1_atten, shifts=(-self.shift_size, -self.shift_size), dims=(2, 3))\n",
    "            shifted_x2 = torch.roll(x2_atten, shifts=(-self.shift_size, -self.shift_size), dims=(2, 3))\n",
    "            x1_atten, x2_atten, _ = self.self_atten_block2(shifted_x1, shifted_x2)\n",
    "            roll_x_self1 = torch.roll(x1_atten, shifts=(self.shift_size, self.shift_size), dims=(2, 3))\n",
    "            roll_x_self2 = torch.roll(x2_atten, shifts=(self.shift_size, self.shift_size), dims=(2, 3))\n",
    "        else:\n",
    "            x1_atten, x2_atten, _ = self.self_atten_block2(x1_atten, x2_atten)\n",
    "            roll_x_self1 = x1_atten\n",
    "            roll_x_self2 = x2_atten\n",
    "        # -------------------------------------\n",
    "        # cross attention\n",
    "        if self.depth_cross > 0:\n",
    "            out, x_self1, x_self2, x_cross1, x_cross2 = self.cross_atten_block(roll_x_self1, roll_x_self2, paddings)\n",
    "        else:\n",
    "            out = roll_x_self1 + roll_x_self2\n",
    "            x_self1, x_self2, x_cross1, x_cross2 = roll_x_self1, roll_x_self2, roll_x_self1, roll_x_self2\n",
    "        # -------------------------------------\n",
    "        # recons\n",
    "        return out, x1_a, x2_a, roll_x_self1, roll_x_self2, x_cross1, x_cross2"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- encoding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
